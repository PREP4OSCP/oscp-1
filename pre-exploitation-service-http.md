# HTTP
## Basic Commands
```
curl -i 10.11.1.71
curl -i -L 10.11.1.71
```
A few things we like to check are : comments (as these wouldn't be seen when rendered), 
the web application name / version, and links to other pages/domains.  

1. Web Application (Internal & External Links)  
```
curl 10.11.1.71 -s -L | grep "title\|href" | sed -e 's/^[[:space:]]*//'
```

2. Web Application (HTML Render)  
So thats all great, but let's now see what the web page renders like.
At this point we can start up iceweasel/firefox/chrome to look at the page...
instead, let's stick with command line for the time being. Welcome "html2text".  

```
curl 10.11.1.71 -s -L | html2text -width '99' | uniq
```

3. Web Application (Social Networks)  
> This can also be re-enforced by one of the social network links.  

looking at the diff social media links in order to verify the software beeing in use.  

4. Web Application (Accessing The Source Code)  
> remember to look at the README.md file, it'll reveal additional info.  

We download the web application/software in order to look at the code. Search the 'changelog' for `bug/security fix`, 
that might reveal important `exploitable` issues.  
```
curl 10.11.1.71/README.md
```

5. Web Application (Hidden)  
> Robots(.txt) vs Spiders  

A quick check to see if there's anything the system administrator wouldn't want a Internet spider to index:
Note: For common/default values to look/check for: https://github.com/h5bp/html5-boilerplate

```
curl 10.11.1.71/robots.txt -s | html2text
```

6. URL Brute Force (General)  
Utilizing diff tools and wordlists to 'guess' existing folders/files  

Some of the mentioned tools come with their own wordlists with them (such as DirB & wfuzz) which have commonly found URLs - and you can mix and match the tools to the wordlists.
However, there is a dedicated project called "SecList" which aims to cover as many general/generic wordlists as possible (for every topic). Its worth having a quick explore:

    DirB - /usr/share/dirb/wordlists/
    wfuzz - /usr/share/wfuzz/wordlist/
    SecList - /usr/share/seclists/

```
gobuster -u http://10.11.1.71/ -w /usr/share/seclists/Discovery/Web-Content/common.txt -s '200,204,301,302,307,403,500' -e
```

7. URL Brute Force (CGI)  
So next on our list, another wordlist to try! As we have already done a wordlist of common values, let's use what we learnt from it and start to get a specific/specialize, by targeting CGI URLs.  

```
gobuster -u http://10.11.1.71/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s '200,204,301,302,307,403,500' -e
gobuster -u http://10.11.1.71/ -w /usr/share/seclists/Discovery/Web-Content/CGIs.txt -s '200,204,403,500' -e

```
